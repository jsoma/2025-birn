{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4be30740-4fbd-406b-ab06-625eb122b613",
   "metadata": {},
   "source": [
    "# Real life pandas work with Kosovo privatization PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c13cd7f-8381-4a36-91fd-abf59d2cc4c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Let's get started!\n",
    "\n",
    "We're going to work with [example-bid.pdf](example-bid.pdf), a PDF from the [Privatization Agency of Kosovo](https://www.pak-ks.org/page.aspx?id=1%2C106). The agency is responsibile for the privatization of socially owned enterprises, which appears to be done through a competitive bidding process.\n",
    "\n",
    "The data is *almost* a spreadsheet, but... not exactly. When you have data in a PDF, unfortunately you can't just use `pd.read_pdf` (it doesn't exist!).\n",
    "\n",
    "You can *try* if you want, but you'll be disappointed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31c8c8-deda-4fcd-a770-03e519f26643",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pdf(\"example-bid.pdf\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba83d0a-40e8-4c62-8a33-a815c2ab80cb",
   "metadata": {},
   "source": [
    "To get the data out of the documents, we're going to use [Natural PDF](https://jsoma.github.io/natural-pdf/), a very nice Python tool that helps you extract information from PDFs. I'm biased because I made it!\n",
    "\n",
    "We'll start by looking at the **first page.** If we can make it work on the first page, we can make it work on the rest of the pages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f61a74-a5dd-4d69-8c0b-ae4d5ee7174c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from natural_pdf import PDF\n",
    "\n",
    "pdf = PDF(\"example-bid.pdf\")\n",
    "page = pdf.pages[0]\n",
    "page.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1eec18-5b85-4cd3-8b34-4714474f076a",
   "metadata": {},
   "source": [
    "Sometimes you can just ask Natural PDF to give you the entire table. Does that work for us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c94fd38-8de9-495c-b04b-70bdbe1edb18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = page.extract_table().to_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f9268-138e-4e4e-88c0-c2dedc5158cc",
   "metadata": {},
   "source": [
    "...not really, the headers are kind of weird and spread out across two rows. And there are also a few empty columns, full of `<NA>` again and again.\n",
    "\n",
    "Why might that be? Probably something with the headers: **we should look at the PDF to see**. I'll grab the yellowish rectangle and examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd14ff6b-c038-44a9-b7e5-ebf0850a0121",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_header = page.find('rect[fill~=yellow]')\n",
    "table_header.show(crop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b0e43-0ec3-4bd8-91bf-d3983602eee0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Two things:\n",
    "\n",
    "1. The horizontal lines are confusing the table extractor\n",
    "2. The `Tre Ofertuesit me Ã§mim mÃ« tÃ« lartÃ«` text is getting in the way.\n",
    "\n",
    "We can fix it most of the way by just removing `Tre Ofertuesit me Ã§mim mÃ« tÃ« lartÃ«`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04108a-9f0f-4dd9-80ca-8babab37be57",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Find the Tre Ofert text and exclude it from the page\n",
    "    page.find('text:contains(Tre Ofert)').exclude()\n",
    "except:\n",
    "    # Already excluded\n",
    "    pass\n",
    "\n",
    "# Grab the column names, combine any multi-line ones\n",
    "column_names = (\n",
    "    table_header\n",
    "    .find_all('text')\n",
    "    .dissolve(vertical=True)\n",
    "    .extract_each_text(newlines=False, order='ltr')\n",
    ")\n",
    "print(\"Column names are\", column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a7647-d24a-4e17-b62a-5e693f0af89e",
   "metadata": {},
   "source": [
    "Ok, headers look good, but what about the data? We can then ask for the table from **below the headers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8bbfa5-18f4-48b7-bbe4-f5f8098f9332",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take a look at it\n",
    "table_header.below().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b32925-ce8b-4f67-8396-d1a65a533b36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now actually grab the data\n",
    "df = table_header.below().extract_table().to_df(header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93177077-ebac-442f-97dc-513e97f8871b",
   "metadata": {},
   "source": [
    "It's this kind of **multi-step process** that involving thinking and looking that AI isn't very good at yet. It can *kind of* do it *sometimes* but I'd never rely on it (and Natural PDF is too new for it to know much about it).\n",
    "\n",
    "Our code isn't done yet:\n",
    "\n",
    "1. We still have those awful `<NA>` columns\n",
    "2. We don't have the column headers in our dataframe\n",
    "3. Our column headers repeat for the top 3 bidders\n",
    "4. If we want to be very picky about our analysis, we can't do math with `â‚¬1,111,000`, we need it to be an *actual number*.\n",
    "5. And this is still only the first page!\n",
    "\n",
    "While we *could* try to figure this out ourselves, pay attention to what's going on: with the exception of the \"do this on every page,\" **our problems are all pandas work.** It's all manipulating dataframes, something that ChatGPT and friends is very very good at.\n",
    "\n",
    "And they're also very good at taking a little sample of code and expanding it out over a bigger universe. So let's try that out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b6ce92-d459-4298-bdf3-f1ef7982cdfe",
   "metadata": {},
   "source": [
    "## Step one: Put all your code in one cell\n",
    "\n",
    "If we're going to ask for help from ChatGPT, we need to put the code all in one place. Run it and make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70934849-4963-49ba-bd92-9acab292822c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from natural_pdf import PDF\n",
    "\n",
    "pdf = PDF(\"example-bid.pdf\")\n",
    "page = pdf.pages[0]\n",
    "\n",
    "# Get the table headers\n",
    "\n",
    "table_header = page.find('rect[fill~=yellow]')\n",
    "\n",
    "try:\n",
    "    # Find the Tre Ofert text and exclude it from the page\n",
    "    table_header.find('text:contains(Tre Ofert)').exclude()\n",
    "except:\n",
    "    # Already excluded\n",
    "    pass\n",
    "\n",
    "# Grab the column names, combine any multi-line ones\n",
    "column_names = (\n",
    "    table_header\n",
    "    .find_all('text')\n",
    "    .dissolve(vertical=True)\n",
    "    .extract_each_text(newlines=False, order='ltr')\n",
    ")\n",
    "\n",
    "print(\"Columns are\", column_names)\n",
    "\n",
    "# Get the table itself\n",
    "df = table_header.below().extract_table().to_df(header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553eafdd-8cb2-47f0-bd2d-09f13f13c300",
   "metadata": {},
   "source": [
    "# Talking to ChatGPT\n",
    "\n",
    "## How to ask for help\n",
    "\n",
    "A good prompt has five parts:\n",
    "\n",
    "1. An overall view of what I'm doing (and how)\n",
    "2. My problem(s), in as much detail as possible.\n",
    "3. What I want done or changed or fixed. Be specific! You're talking to an *enthusiastic but distractable intern.*\n",
    "4. My current code\n",
    "5. The output\n",
    "\n",
    "## Our prompt\n",
    "\n",
    "```\n",
    "In the code below I am scraping a table from the first page of a PDF in a Jupyter notebook. It works, but I need some improvements:\n",
    "\n",
    "1. The bidding price columns are money with euro and commas symbols in them. Clean them up so I can analyze them.\n",
    "2. Some of the columns don't have any data in them. Remove those columns (ONLY those NA ones - not zeroes, zeroes are ok)\n",
    "3. I want the nice column names in the dataframe, but you need to remove the \"bad\" columns before you assign names\n",
    "4. The bidder and the bid price column names have duplicate names for the first, second, and third place bidders. Add numbers after the column name to keep it organized.\n",
    "5. There are multiple pages on the pdf and I want them combined into one dataframe. As you do this I want to see a progress bar.\n",
    "\n",
    "The code should be \"safe\" with the data - if anything unexpected happens, provide a warning or an error to show up so we don't lose data. Here is my current code:\n",
    "\n",
    "from natural_pdf import PDF\n",
    "\n",
    "pdf = PDF(\"example-bid.pdf\")\n",
    "page = pdf.pages[0]\n",
    "\n",
    "table_header = page.find('rect[fill~=yellow]')\n",
    "\n",
    "try:\n",
    "    # Find the Tre Ofert text and exclude it from the page\n",
    "    table_header.find('text:contains(Tre Ofert)').exclude()\n",
    "except:\n",
    "    # Already excluded\n",
    "    pass\n",
    "\n",
    "column_names = (\n",
    "    table_header\n",
    "    .find_all('text')\n",
    "    .dissolve(vertical=True)\n",
    "    .extract_each_text(newlines=False, order='ltr')\n",
    ")\n",
    "\n",
    "print(\"Columns are\", column_names)\n",
    "\n",
    "df = table_header.below().extract_table().to_df(header=None)\n",
    "df.head()\n",
    "\n",
    "Here is the output:\n",
    "\n",
    "Columns are ['Nr.', 'NjÃ«sitÃ«', 'AKP ID', 'Ã‡mimi mÃ« i LartÃ«', 'Ofertuesi', 'Ã‡mimi', 'Ofertuesi', 'Ã‡mimi', 'Ofertuesi', 'Ã‡mimi']\n",
    "\n",
    "0\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\n",
    "\n",
    "0\t1\tNjÃ«sia nr.01: Agrokultura Toka nÃ« Gjilan (Lot L)\tGJI004\tâ‚¬15,127\tL106\tâ‚¬15,127\t<NA>\t0\tâ‚¬0\t<NA>\t0\tâ‚¬0\n",
    "\n",
    "1\t2\tNjÃ«sia nr.02: Agrokultura Toka nÃ« Gjilan (Lot M)\tGJI004\tâ‚¬0\t0\tâ‚¬0\t<NA>\t0\tâ‚¬0\t<NA>\t0\tâ‚¬0\n",
    "\n",
    "2\t3\tNjÃ«sia nr.03: Pasuria BujqÃ«sore Toka nÃ« Bibaj ...\tGJI011\tâ‚¬69,611\tL76\tâ‚¬69,611\t<NA>\tL34\tâ‚¬58,000\t<NA>\tL63\tâ‚¬12,222\n",
    "\n",
    "3\t4\tNjÃ«sia nr.04: Pasuria BujqÃ«sore Toka nÃ« Ferizaj\tGJI011\tâ‚¬1,111,000\tL133\tâ‚¬1,111,000\t<NA>\tL58\tâ‚¬666,666\t<NA>\tL55\tâ‚¬513,501\n",
    "\n",
    "4\t5\tNjÃ«sia nr.05: Pasuria BujqÃ«sore Toka nÃ« PojatÃ«...\tGJI011\tâ‚¬3,892\tL57\tâ‚¬3,892\t<NA>\t0\tâ‚¬0\t<NA>\t0\tâ‚¬0\n",
    "```\n",
    "\n",
    "Send the code to ChatGPT and you'll (hopefully!) get a working result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb2f77-f139-42c6-954d-29092c85f9c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "from natural_pdf import PDF\n",
    "\n",
    "def clean_money(value):\n",
    "    \"\"\"Convert euro strings like 'â‚¬1,111,000' to floats. Handle bad values safely.\"\"\"\n",
    "    try:\n",
    "        if pd.isna(value):\n",
    "            return pd.NA\n",
    "        # Remove euro sign and commas, then cast\n",
    "        return float(str(value).replace(\"â‚¬\", \"\").replace(\",\", \"\").strip())\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Could not parse money value '{value}': {e}\")\n",
    "        return pd.NA\n",
    "\n",
    "# Load PDF\n",
    "pdf = PDF(\"example-bid.pdf\")\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for page in tqdm(pdf.pages, desc=\"Processing pages\"):\n",
    "    # --- Find table header\n",
    "    table_header = page.find('rect[fill~=yellow]')\n",
    "    try:\n",
    "        table_header.find('text:contains(Tre Ofert)').exclude()\n",
    "    except Exception:\n",
    "        pass  # already excluded\n",
    "\n",
    "    # --- Extract raw column names\n",
    "    column_names = (\n",
    "        table_header\n",
    "        .find_all('text')\n",
    "        .dissolve(vertical=True)\n",
    "        .extract_each_text(newlines=False, order='ltr')\n",
    "    )\n",
    "\n",
    "    # --- Extract table content\n",
    "    df = table_header.below().extract_table().to_df(header=None)\n",
    "\n",
    "    # --- Drop fully empty columns BEFORE assigning names\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # Safety check: column count should match\n",
    "    if len(df.columns) != len(column_names):\n",
    "        warnings.warn(\n",
    "            f\"Column name count ({len(column_names)}) != DataFrame cols ({len(df.columns)}). \"\n",
    "            \"Truncating to match.\"\n",
    "        )\n",
    "        column_names = column_names[:len(df.columns)]\n",
    "\n",
    "    # --- Handle duplicate names (Ofertuesi/Ã‡mimi appear multiple times)\n",
    "    new_columns = []\n",
    "    seen = {}\n",
    "    for col in column_names:\n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            new_columns.append(f\"{col}_{seen[col]}\")\n",
    "        else:\n",
    "            seen[col] = 1\n",
    "            new_columns.append(f\"{col}_1\" if col in [\"Ofertuesi\", \"Ã‡mimi\"] else col)\n",
    "    df.columns = new_columns\n",
    "\n",
    "    # --- Clean up money columns\n",
    "    for col in df.columns:\n",
    "        if \"Ã‡mimi\" in col or \"LartÃ«\" in col:\n",
    "            df[col] = df[col].apply(clean_money)\n",
    "\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# --- Combine all pages\n",
    "final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ebd579-bc0d-4914-8787-715b5da43824",
   "metadata": {},
   "source": [
    "## Round two: all of the PDFs\n",
    "\n",
    "Another prompt:\n",
    "\n",
    "```\n",
    "I have many pdfs in the folder bid-pdfs/. I would like to similarly read in all of those PDFs and turn them into one big dataframe using the code above. I'd like there to be two new columns, too, 'source_file' and 'page_number'. Save all the errors/warnings up until the end and display them organized by the file name.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ff4b2-b938-43a3-8b39-d6a6298fbc17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from natural_pdf import PDF\n",
    "\n",
    "# --- Helper to clean money values\n",
    "def clean_money(value):\n",
    "    try:\n",
    "        if pd.isna(value):\n",
    "            return pd.NA\n",
    "        return float(str(value).replace(\"â‚¬\", \"\").replace(\",\", \"\").strip())\n",
    "    except Exception as e:\n",
    "        return f\"PARSE_ERROR: {value} ({e})\"  # leave marker instead of silently dropping\n",
    "\n",
    "# --- Storage\n",
    "all_dfs = []\n",
    "error_log = {}\n",
    "\n",
    "# --- Loop over all PDFs\n",
    "pdf_files = list(Path(\"bid-pdfs\").glob(\"*.pdf\"))\n",
    "\n",
    "for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "    file_errors = []\n",
    "    try:\n",
    "        pdf = PDF(str(pdf_path))\n",
    "\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            try:\n",
    "                # --- Locate header\n",
    "                table_header = page.find('rect[fill~=yellow]')\n",
    "                try:\n",
    "                    table_header.find('text:contains(Tre Ofert)').exclude()\n",
    "                except Exception:\n",
    "                    pass  # already excluded\n",
    "\n",
    "                # --- Extract headers\n",
    "                column_names = (\n",
    "                    table_header\n",
    "                    .find_all('text')\n",
    "                    .dissolve(vertical=True)\n",
    "                    .extract_each_text(newlines=False, order='ltr')\n",
    "                )\n",
    "\n",
    "                # --- Extract table\n",
    "                df = table_header.below().extract_table().to_df(header=None)\n",
    "\n",
    "                # --- Drop fully empty columns BEFORE naming\n",
    "                df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "                if len(df.columns) != len(column_names):\n",
    "                    file_errors.append(\n",
    "                        f\"[Page {page_num}] Column mismatch: \"\n",
    "                        f\"{len(column_names)} names vs {len(df.columns)} cols\"\n",
    "                    )\n",
    "                    column_names = column_names[:len(df.columns)]\n",
    "\n",
    "                # --- Fix duplicate names\n",
    "                new_columns = []\n",
    "                seen = {}\n",
    "                for col in column_names:\n",
    "                    if col in seen:\n",
    "                        seen[col] += 1\n",
    "                        new_columns.append(f\"{col}_{seen[col]}\")\n",
    "                    else:\n",
    "                        seen[col] = 1\n",
    "                        new_columns.append(f\"{col}_1\" if col in [\"Ofertuesi\", \"Ã‡mimi\"] else col)\n",
    "                df.columns = new_columns\n",
    "\n",
    "                # --- Clean money columns\n",
    "                for col in df.columns:\n",
    "                    if \"Ã‡mimi\" in col or \"LartÃ«\" in col:\n",
    "                        df[col] = df[col].apply(clean_money)\n",
    "\n",
    "                # --- Add metadata columns\n",
    "                df[\"source_file\"] = pdf_path.name\n",
    "                df[\"page_number\"] = page_num\n",
    "\n",
    "                all_dfs.append(df)\n",
    "\n",
    "            except Exception as e:\n",
    "                file_errors.append(f\"[Page {page_num}] ERROR: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        file_errors.append(f\"[File load error] {e}\")\n",
    "\n",
    "    if file_errors:\n",
    "        error_log[pdf_path.name] = file_errors\n",
    "\n",
    "# --- Combine everything\n",
    "if all_dfs:\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "else:\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "# --- Show errors grouped by file\n",
    "if error_log:\n",
    "    print(\"\\n===== ERROR / WARNING LOG =====\\n\")\n",
    "    for fname, errs in error_log.items():\n",
    "        print(f\"File: {fname}\")\n",
    "        for err in errs:\n",
    "            print(\"   -\", err)\n",
    "        print()\n",
    "else:\n",
    "    print(\"No errors or warnings ðŸŽ‰\")\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba1287-e21a-48ae-a6a0-1411f57cde5c",
   "metadata": {},
   "source": [
    "## Download and analyze\n",
    "\n",
    "Even if we know what we're looking for, it can always be nice to move over to our friend [ChatGPT](https://chatgpt.com/) to help us analyze the data. Like we saw before: **AI can accurately answer questions with spreadsheets that it can't with PDFs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5bd45-53de-4d17-98bc-192bb2395e27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# or maybe it's called df, who knows!\n",
    "final_df.to_csv(\"bids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494bbf28-b36b-4378-a335-1880ae70d7e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, display\n",
    "\n",
    "filepath = 'bids.csv'\n",
    "\n",
    "try:\n",
    "    # Try to import Colab's download helper\n",
    "    from google.colab import files\n",
    "    files.download(filepath)\n",
    "except ImportError:\n",
    "    # Fallback: display a clickable link inside the notebook\n",
    "    display(FileLink(filepath))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (natural-pdf project venv)",
   "language": "python",
   "name": "natural-pdf-project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "workshop": {
   "data_files": [
    "bid-pdfs/*.pdf",
    "example-bid.pdf"
   ],
   "description": "Working with data in the real world is an awful, awful experience. Let's work on some spreadsheets about Kosovo's privatisation efforts.",
   "install": "tqdm pandas natural_pdf",
   "links": [
    {
     "name": "Bid Reports page",
     "url": "https://www.pak-ks.org/page.aspx?id=1%2C106"
    },
    {
     "description": "A Python tool for analyzing PDFs",
     "name": "Natural PDF",
     "url": "https://jsoma.github.io/natural-pdf/"
    }
   ],
   "order": 3,
   "title": "Extracting Bid Data from PDFs"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
